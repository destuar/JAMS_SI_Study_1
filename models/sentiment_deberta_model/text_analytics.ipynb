{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text analytics for DEI stance & purchase intentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import 'combined_sentiment_annotations.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ../../data/annotate/complete/combined_sentiment_annotations.csv\n",
      "DataFrame shape: (1000, 25)\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>post_date</th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>comment_date</th>\n",
       "      <th>comment_type</th>\n",
       "      <th>reaction_count</th>\n",
       "      <th>before_DEI</th>\n",
       "      <th>has_DEI</th>\n",
       "      <th>...</th>\n",
       "      <th>full_text</th>\n",
       "      <th>relevance</th>\n",
       "      <th>stance_dei_reviewer_01</th>\n",
       "      <th>purchase_intention_reviewer_01</th>\n",
       "      <th>stance_dei_reviewer_02</th>\n",
       "      <th>purchase_intention_reviewer_02</th>\n",
       "      <th>stance_dei_average</th>\n",
       "      <th>purchase_intention_average</th>\n",
       "      <th>stance_dei_label</th>\n",
       "      <th>purchase_intention_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Target</td>\n",
       "      <td>2/2/25 11:06</td>\n",
       "      <td>Y29tbWVudDoxMTcxMzM2NDkxMDE4NDA3XzExMzM1OTk4OT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The audacity</td>\n",
       "      <td>2/7/25</td>\n",
       "      <td>initial</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>the audacity</td>\n",
       "      <td>0</td>\n",
       "      <td>0 (Neutral/Unclear towards DEI)</td>\n",
       "      <td>0 (Neutral/Unclear/No PI)</td>\n",
       "      <td>0 (Neutral/Unclear towards DEI)</td>\n",
       "      <td>0 (Neutral/Unclear/No PI)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Costco</td>\n",
       "      <td>1/13/25 11:00</td>\n",
       "      <td>Y29tbWVudDoxMDIwNjM1NjYzNDI3OTMwXzEyNjM0Njg2MD...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yummmmm.,</td>\n",
       "      <td>1/17/25</td>\n",
       "      <td>initial</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>yummmmm.,</td>\n",
       "      <td>0</td>\n",
       "      <td>0 (Neutral/Unclear towards DEI)</td>\n",
       "      <td>0 (Neutral/Unclear/No PI)</td>\n",
       "      <td>0 (Neutral/Unclear towards DEI)</td>\n",
       "      <td>0 (Neutral/Unclear/No PI)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Costco</td>\n",
       "      <td>2/9/25 9:00</td>\n",
       "      <td>Y29tbWVudDoxMDM5NjIyNDU0ODYyNTg0XzM2OTUyNzExMj...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How much</td>\n",
       "      <td>2/16/25</td>\n",
       "      <td>initial</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>how much</td>\n",
       "      <td>0</td>\n",
       "      <td>0 (Neutral/Unclear towards DEI)</td>\n",
       "      <td>0 (Neutral/Unclear/No PI)</td>\n",
       "      <td>0 (Neutral/Unclear towards DEI)</td>\n",
       "      <td>0 (Neutral/Unclear/No PI)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Costco</td>\n",
       "      <td>2/16/25 9:00</td>\n",
       "      <td>Y29tbWVudDoxMDQ1NjAzNjA0MjY0NDY5XzkwODU0Mjg4ND...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>That looks so good!!! It also looks like the l...</td>\n",
       "      <td>2/23/25</td>\n",
       "      <td>initial</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>that looks so good!!! it also looks like the l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0 (Neutral/Unclear towards DEI)</td>\n",
       "      <td>1 (Buy/Positive)</td>\n",
       "      <td>0 (Neutral/Unclear towards DEI)</td>\n",
       "      <td>1 (Buy/Positive)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delta</td>\n",
       "      <td>2/13/25 9:30</td>\n",
       "      <td>Y29tbWVudDoxMDIxMzAxOTAwMDE4NzIyXzIwMDQ2MzM4Mj...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I have a friend who wanted to be a pilot for D...</td>\n",
       "      <td>2/15/25</td>\n",
       "      <td>initial</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>i have a friend who wanted to be a pilot for d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0 (Neutral/Unclear towards DEI)</td>\n",
       "      <td>0 (Neutral/Unclear/No PI)</td>\n",
       "      <td>0 (Neutral/Unclear towards DEI)</td>\n",
       "      <td>0 (Neutral/Unclear/No PI)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Costco</td>\n",
       "      <td>2/11/25 9:00</td>\n",
       "      <td>Y29tbWVudDoxMDQxNzYyOTI3OTgxODcwXzExNTU4NjE1OT...</td>\n",
       "      <td>Y29tbWVudDoxMDQxNzYyOTI3OTgxODcwXzgyODIwNTI2Mj...</td>\n",
       "      <td>another person who doesn’t understand DEI</td>\n",
       "      <td>2/16/25</td>\n",
       "      <td>reply</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>how much money are your executives getting to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1 (Anti-DEI)</td>\n",
       "      <td>0 (Neutral/Unclear/No PI)</td>\n",
       "      <td>-1 (Anti-DEI)</td>\n",
       "      <td>0 (Neutral/Unclear/No PI)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  company_name      post_date  \\\n",
       "0       Target   2/2/25 11:06   \n",
       "1       Costco  1/13/25 11:00   \n",
       "2       Costco    2/9/25 9:00   \n",
       "3       Costco   2/16/25 9:00   \n",
       "4        Delta   2/13/25 9:30   \n",
       "5       Costco   2/11/25 9:00   \n",
       "\n",
       "                                                  id  \\\n",
       "0  Y29tbWVudDoxMTcxMzM2NDkxMDE4NDA3XzExMzM1OTk4OT...   \n",
       "1  Y29tbWVudDoxMDIwNjM1NjYzNDI3OTMwXzEyNjM0Njg2MD...   \n",
       "2  Y29tbWVudDoxMDM5NjIyNDU0ODYyNTg0XzM2OTUyNzExMj...   \n",
       "3  Y29tbWVudDoxMDQ1NjAzNjA0MjY0NDY5XzkwODU0Mjg4ND...   \n",
       "4  Y29tbWVudDoxMDIxMzAxOTAwMDE4NzIyXzIwMDQ2MzM4Mj...   \n",
       "5  Y29tbWVudDoxMDQxNzYyOTI3OTgxODcwXzExNTU4NjE1OT...   \n",
       "\n",
       "                                           parent_id  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5  Y29tbWVudDoxMDQxNzYyOTI3OTgxODcwXzgyODIwNTI2Mj...   \n",
       "\n",
       "                                        comment_text comment_date  \\\n",
       "0                                       The audacity       2/7/25   \n",
       "1                                          Yummmmm.,      1/17/25   \n",
       "2                                           How much      2/16/25   \n",
       "3  That looks so good!!! It also looks like the l...      2/23/25   \n",
       "4  I have a friend who wanted to be a pilot for D...      2/15/25   \n",
       "5          another person who doesn’t understand DEI      2/16/25   \n",
       "\n",
       "  comment_type  reaction_count  before_DEI  has_DEI  ...  \\\n",
       "0      initial               0           0        0  ...   \n",
       "1      initial               0           1        1  ...   \n",
       "2      initial               0           0        1  ...   \n",
       "3      initial               1           0        1  ...   \n",
       "4      initial               0           0        1  ...   \n",
       "5        reply              19           0        1  ...   \n",
       "\n",
       "                                           full_text  relevance  \\\n",
       "0                                       the audacity          0   \n",
       "1                                          yummmmm.,          0   \n",
       "2                                           how much          0   \n",
       "3  that looks so good!!! it also looks like the l...          0   \n",
       "4  i have a friend who wanted to be a pilot for d...          0   \n",
       "5  how much money are your executives getting to ...          1   \n",
       "\n",
       "            stance_dei_reviewer_01 purchase_intention_reviewer_01  \\\n",
       "0  0 (Neutral/Unclear towards DEI)      0 (Neutral/Unclear/No PI)   \n",
       "1  0 (Neutral/Unclear towards DEI)      0 (Neutral/Unclear/No PI)   \n",
       "2  0 (Neutral/Unclear towards DEI)      0 (Neutral/Unclear/No PI)   \n",
       "3  0 (Neutral/Unclear towards DEI)               1 (Buy/Positive)   \n",
       "4  0 (Neutral/Unclear towards DEI)      0 (Neutral/Unclear/No PI)   \n",
       "5                    -1 (Anti-DEI)      0 (Neutral/Unclear/No PI)   \n",
       "\n",
       "            stance_dei_reviewer_02 purchase_intention_reviewer_02  \\\n",
       "0  0 (Neutral/Unclear towards DEI)      0 (Neutral/Unclear/No PI)   \n",
       "1  0 (Neutral/Unclear towards DEI)      0 (Neutral/Unclear/No PI)   \n",
       "2  0 (Neutral/Unclear towards DEI)      0 (Neutral/Unclear/No PI)   \n",
       "3  0 (Neutral/Unclear towards DEI)               1 (Buy/Positive)   \n",
       "4  0 (Neutral/Unclear towards DEI)      0 (Neutral/Unclear/No PI)   \n",
       "5                    -1 (Anti-DEI)      0 (Neutral/Unclear/No PI)   \n",
       "\n",
       "   stance_dei_average purchase_intention_average stance_dei_label  \\\n",
       "0                 0.0                        0.0                0   \n",
       "1                 0.0                        0.0                0   \n",
       "2                 0.0                        0.0                0   \n",
       "3                 0.0                        1.0                0   \n",
       "4                 0.0                        0.0                0   \n",
       "5                -1.0                        0.0               -1   \n",
       "\n",
       "  purchase_intention_label  \n",
       "0                        0  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        1  \n",
       "4                        0  \n",
       "5                        0  \n",
       "\n",
       "[6 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define file path relative to the notebook or project root\n",
    "csv_file_path = '../../data/annotate/complete/combined_sentiment_annotations.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "print(f\"Successfully loaded {csv_file_path}\")\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(\"First 5 rows:\")\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract numeric score from label string (handles potential errors)\n",
    "def extract_score(label):\n",
    "    if pd.isna(label) or not isinstance(label, str):\n",
    "        return np.nan\n",
    "    # Match integers (positive, negative, or zero) at the beginning of the string\n",
    "    match = re.match(r\"^(-?\\d+)\", label)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return np.nan # Return NaN if no number is found at the beginning\n",
    "\n",
    "reviewer_cols = [\n",
    "    'stance_dei_reviewer_01', 'purchase_intention_reviewer_01',\n",
    "    'stance_dei_reviewer_02', 'purchase_intention_reviewer_02'\n",
    "    ]\n",
    "\n",
    "# Check if columns exist before processing\n",
    "missing_cols = [col for col in reviewer_cols if col not in df.columns]\n",
    "\n",
    "df['stance_dei_score_r1'] = df['stance_dei_reviewer_01'].apply(extract_score)\n",
    "df['pi_score_r1'] = df['purchase_intention_reviewer_01'].apply(extract_score)\n",
    "df['stance_dei_score_r2'] = df['stance_dei_reviewer_02'].apply(extract_score)\n",
    "df['pi_score_r2'] = df['purchase_intention_reviewer_02'].apply(extract_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for final 'stance_dei_label':\n",
      "stance_dei_label\n",
      " 0    784\n",
      " 1    126\n",
      "-1     90\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Counts for final 'purchase_intention_label':\n",
      "purchase_intention_label\n",
      " 0    762\n",
      "-1    127\n",
      " 1    111\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "final_label_cols = ['stance_dei_label', 'purchase_intention_label']\n",
    "\n",
    "print(\"Counts for final 'stance_dei_label':\")\n",
    "dei_counts = df['stance_dei_label'].value_counts(dropna=False) # Include NaNs if any\n",
    "print(dei_counts)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Counts for final 'purchase_intention_label':\")\n",
    "pi_counts = df['purchase_intention_label'].value_counts(dropna=False) # Include NaNs if any\n",
    "print(pi_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculate Cohen's Kappa for dual-coded annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Kappa for DEI Stance using 1000 complete pairs of ratings.\n",
      "Cohen's Kappa for DEI Stance: 0.8173\n",
      "\n",
      "\n",
      "Calculating Kappa for Purchase Intention using 1000 complete pairs of ratings.\n",
      "Cohen's Kappa for Purchase Intention: 0.7385\n"
     ]
    }
   ],
   "source": [
    "df_kappa_dei = df[['stance_dei_score_r1', 'stance_dei_score_r2']].dropna()\n",
    "print(f\"Calculating Kappa for DEI Stance using {len(df_kappa_dei)} complete pairs of ratings.\")\n",
    "\n",
    "kappa_dei = cohen_kappa_score(df_kappa_dei['stance_dei_score_r1'], df_kappa_dei['stance_dei_score_r2'])\n",
    "print(f\"Cohen's Kappa for DEI Stance: {kappa_dei:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "df_kappa_pi = df[['pi_score_r1', 'pi_score_r2']].dropna()\n",
    "print(f\"Calculating Kappa for Purchase Intention using {len(df_kappa_pi)} complete pairs of ratings.\")\n",
    "\n",
    "kappa_pi = cohen_kappa_score(df_kappa_pi['pi_score_r1'], df_kappa_pi['pi_score_r2'])\n",
    "print(f\"Cohen's Kappa for Purchase Intention: {kappa_pi:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Labels for Imbalance Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example DEI stance indices: [1 1 1 1 1 0 0 1 1 2]\n",
      "Example PI indices: [1 1 1 2 1 1 0 1 0 0]\n",
      "\n",
      "\n",
      "Unique stance classes found for weighting: [0 1 2]\n",
      "Unique PI classes found for weighting: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Map labels from [-1, 0, 1] to [0, 1, 2] for compatibility with loss functions\n",
    "y_stance_original = df['stance_dei_label'].astype(int).values\n",
    "y_pi_original = df['purchase_intention_label'].astype(int).values\n",
    "\n",
    "# Stance: -1 (Anti) -> 0, 0 (Neutral) -> 1, 1 (Pro) -> 2\n",
    "y_stance_idx = y_stance_original + 1\n",
    "\n",
    "# PI: -1 (Boycott) -> 0, 0 (Neutral) -> 1, 1 (Buy) -> 2\n",
    "y_pi_idx = y_pi_original + 1\n",
    "\n",
    "print(f\"Example DEI stance indices: {y_stance_idx[:10]}\")\n",
    "print(f\"Example PI indices: {y_pi_idx[:10]}\")\n",
    "\n",
    "# Define class names for reporting\n",
    "stance_class_names = [\"anti\", \"neutral\", \"pro\"]\n",
    "pi_class_names = [\"boycott\", \"neutral\", \"buy\"]\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Ensure classes are correctly identified (0, 1, 2)\n",
    "stance_classes = np.unique(y_stance_idx) # Should be [0, 1, 2]\n",
    "print(f\"Unique stance classes found for weighting: {stance_classes}\")\n",
    "pi_classes = np.unique(y_pi_idx) # Should be [0, 1, 2]      \n",
    "print(f\"Unique PI classes found for weighting: {pi_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS backend is available. Using MPS device.\n",
      "Selected device: mps\n"
     ]
    }
   ],
   "source": [
    "# Let's define our PyTorch Device (I am using MPS for Apple Silicon on Macbook M3 Pro)\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    # Check if MPS is available\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS backend is available. Using MPS device.\")\n",
    "elif not torch.backends.mps.is_built():\n",
    "    # Check if MPS is built (required for is_available to be True)\n",
    "    # This case is unlikely if is_available() is False, but good to be explicit\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available because the current PyTorch install was not built with MPS enabled.\")\n",
    "else:\n",
    "    # MPS is built but not available (e.g., OS version issue, though unlikely on modern macOS)\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Using CPU device.\")\n",
    "\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stance Label Counts: {np.int64(0): np.int64(90), np.int64(1): np.int64(784), np.int64(2): np.int64(126)}\n",
      "Created WeightedRandomSampler for Stance DEI.\n",
      "\n",
      "\n",
      "PI Label Counts: {np.int64(0): np.int64(127), np.int64(1): np.int64(762), np.int64(2): np.int64(111)}\n",
      "Created WeightedRandomSampler for Purchase Intention.\n"
     ]
    }
   ],
   "source": [
    "# Now we are going to do some weighted random sampling \n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "# Compute sample weights for Stance DEI (one weight per example)\n",
    "stance_label_counts = {label: count for label, count in zip(*np.unique(y_stance_idx, return_counts=True))}\n",
    "print(f\"Stance Label Counts: {stance_label_counts}\")\n",
    "\n",
    "# Avoid division by zero if a class somehow has 0 counts (shouldn't happen with unique)\n",
    "stance_sample_weights = [1.0 / stance_label_counts[label] if stance_label_counts[label] > 0 else 0 for label in y_stance_idx]\n",
    "\n",
    "# Create the sampler\n",
    "stance_sampler = WeightedRandomSampler(stance_sample_weights, num_samples=len(stance_sample_weights), replacement=True)\n",
    "print(\"Created WeightedRandomSampler for Stance DEI.\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Compute sample weights for Purchase Intention\n",
    "pi_label_counts = {label: count for label, count in zip(*np.unique(y_pi_idx, return_counts=True))}\n",
    "print(f\"PI Label Counts: {pi_label_counts}\")\n",
    "\n",
    "pi_sample_weights = [1.0 / pi_label_counts[label] if pi_label_counts[label] > 0 else 0 for label in y_pi_idx]\n",
    "\n",
    "# Create the sampler\n",
    "pi_sampler = WeightedRandomSampler(pi_sample_weights, num_samples=len(pi_sample_weights), replacement=True)\n",
    "print(\"Created WeightedRandomSampler for Purchase Intention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Format full_text input for DeBERTa-LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| full_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | text_segments                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| how much money are your executives getting to hire dei people i stead of those who can do the job????? greedy much? → another person who doesnt understand dei                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | ['how much money are your executives getting to hire dei people i stead of those who can do the job????? greedy much?', 'another person who doesnt understand dei']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "| we canceled our costco membership after 25 years. we went into the store to ask a full refund! they complied. → oh thank god! so happy to hear this move by the amazing costco is weeding out people like you. → enoch 98: 2for ye men shall put on more adornments than a woman, and coloured garments more than a virgin: in royalty and in grandeur and in power, and in silver and in gold and in purple, and in splendour and in food they shall be poured out as water. 3therefore they shall be wanting in doctrine and wisdom, and they shall perish thereby together with their possessions; and with all their glory and their splendour, and in shame and in slaughter and in great destitution, their spirits shall be cast into the furnace of fire. | ['we canceled our costco membership after 25 years. we went into the store to ask a full refund! they complied.', 'oh thank god! so happy to hear this move by the amazing costco is weeding out people like you.', 'enoch 98: 2for ye men shall put on more adornments than a woman, and coloured garments more than a virgin: in royalty and in grandeur and in power, and in silver and in gold and in purple, and in splendour and in food they shall be poured out as water. 3therefore they shall be wanting in doctrine and wisdom, and they shall perish thereby together with their possessions; and with all their glory and their splendour, and in shame and in slaughter and in great destitution, their spirits shall be cast into the furnace of fire.'] |\n",
      "| lol so many butthurt people keep coming back to the comments section just to get mad and comment more. keep target rent free in your heads! i love it! → no we just keeping our money in our pocket.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | ['lol so many butthurt people keep coming back to the comments section just to get mad and comment more. keep target rent free in your heads! i love it!', 'no we just keeping our money in our pocket.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "| hey costco! could you bring more product like asian foods? → imelda, noted, thanks for sharing your interest!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | ['hey costco! could you bring more product like asian foods?', 'imelda, noted, thanks for sharing your interest!']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "| amazing there were no fatalities. kudos to the crew and delta! → my toughts and my prayers are with the passengers and the crew, i think the crew did a wonderful job                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | ['amazing there were no fatalities. kudos to the crew and delta!', 'my toughts and my prayers are with the passengers and the crew, i think the crew did a wonderful job']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Let's define a lambda function to handle the splitting and NaN cases inline\n",
    "split_lambda = lambda text: [] if pd.isna(text) else text.split(' → ')\n",
    "\n",
    "# Apply the lambda function to create a new column with the list of segments\n",
    "df['text_segments'] = df['full_text'].apply(split_lambda)\n",
    "\n",
    "# Show examples\n",
    "print(df[df['full_text'].str.contains(\" → \", na=False)][['full_text', 'text_segments']].head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sep_token → [SEP]\n",
      "Decoded preview: <REPLY> the audacity </REPLY>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DebertaV2Tokenizer\n",
    "\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "tokenizer  = DebertaV2Tokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "\n",
    "special_tokens = {\"additional_special_tokens\": [\"<CONTEXT>\", \"</CONTEXT>\",\n",
    "                                                \"<REPLY>\", \"</REPLY>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "print(\"sep_token →\", tokenizer.sep_token)\n",
    "\n",
    "df[\"stance_label_idx\"] = y_stance_idx # -1,0,1 → 0,1,2\n",
    "\n",
    "df[\"pi_label_idx\"] = y_pi_idx\n",
    "\n",
    "class SentimentDataset(Dataset):          \n",
    "    def __init__(self, df, tok, max_len):\n",
    "        self.segs  = df[\"text_segments\"].tolist()\n",
    "        self.y_s   = df[\"stance_label_idx\"].tolist()\n",
    "        self.y_p   = df[\"pi_label_idx\"].tolist()\n",
    "        self.tok   = tok; self.max_len = max_len\n",
    "    def __len__(self):  return len(self.segs)\n",
    "    def __getitem__(self, i):\n",
    "        segs = self.segs[i]\n",
    "        txt  = f\"<REPLY> {segs[0]} </REPLY>\" if len(segs)==1 else \\\n",
    "               f\"<CONTEXT> {' </CONTEXT> <CONTEXT> '.join(segs[:-1])} </CONTEXT> \" \\\n",
    "               f\"<REPLY> {segs[-1]} </REPLY>\"\n",
    "        enc  = self.tok(txt, add_special_tokens=False, max_length=self.max_len,\n",
    "                        padding=False, truncation=True,\n",
    "                        return_attention_mask=True, return_tensors=\"pt\")\n",
    "        return { \"input_ids\":      enc[\"input_ids\"].squeeze(),\n",
    "                 \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n",
    "                 \"stance_label\":   torch.tensor(self.y_s[i]),\n",
    "                 \"pi_label\":       torch.tensor(self.y_p[i]) }\n",
    "\n",
    "MAX_LEN = 512\n",
    "preview_ds = SentimentDataset(df, tokenizer, MAX_LEN)\n",
    "print(\"Decoded preview:\", tokenizer.decode(preview_ds[0][\"input_ids\"],\n",
    "                                           skip_special_tokens=False))\n",
    "\n",
    "def join_segments(segs):\n",
    "    \"\"\"\n",
    "    Re‑assemble the list returned by `split_lambda` into the exact text\n",
    "    string that will be fed to the tokenizer later on.\n",
    "    \"\"\"\n",
    "    if not segs:                         # blank row guard\n",
    "        return \"\"\n",
    "    if len(segs) == 1:                   # only a reply (no parents)\n",
    "        return f\"<REPLY> {segs[0]} </REPLY>\"\n",
    "    context = \" </CONTEXT> <CONTEXT> \".join(segs[:-1])\n",
    "    reply   = segs[-1]\n",
    "    return f\"<CONTEXT> {context} </CONTEXT> <REPLY> {reply} </REPLY>\"\n",
    "\n",
    "df[\"joined_text\"] = df[\"text_segments\"].apply(join_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LLM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on ➜ mps\n",
      "\n",
      "trainable params: 3,145,728 || all params: 437,060,608 || trainable%: 0.7197\n",
      "\n",
      "🟢 Epoch 1/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad21a5e8c3da4542b34be655f5f93bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion (stance)\n",
      " [[  0  14   1]\n",
      " [  2 159   0]\n",
      " [  0  24   0]] \n",
      "Confusion (PI)\n",
      " [[ 19   0   1]\n",
      " [157   0   1]\n",
      " [ 22   0   0]] \n",
      "stance F1 0.296   pi F1 0.058\n",
      "\n",
      "🟢 Epoch 2/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cd968524e84261b55bbbc67c9862ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion (stance)\n",
      " [[  0  14   1]\n",
      " [  2 158   1]\n",
      " [  0  24   0]] \n",
      "Confusion (PI)\n",
      " [[ 19   0   1]\n",
      " [156   0   2]\n",
      " [ 22   0   0]] \n",
      "stance F1 0.295   pi F1 0.058\n",
      "\n",
      "🟢 Epoch 3/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36eea49dc5414d1e9466171dd16650ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion (stance)\n",
      " [[ 14   0   1]\n",
      " [159   1   1]\n",
      " [ 24   0   0]] \n",
      "Confusion (PI)\n",
      " [[ 19   0   1]\n",
      " [157   0   1]\n",
      " [ 22   0   0]] \n",
      "stance F1 0.048   pi F1 0.058\n",
      "\n",
      "🟢 Epoch 4/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1bc2806ef3445a08a8d2c86bba2e2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion (stance)\n",
      " [[ 14   0   1]\n",
      " [158   0   3]\n",
      " [ 24   0   0]] \n",
      "Confusion (PI)\n",
      " [[ 19   0   1]\n",
      " [157   0   1]\n",
      " [ 22   0   0]] \n",
      "stance F1 0.044   pi F1 0.058\n",
      "\n",
      "🟢 Epoch 5/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2c9048c24f4d459f5dfe7da524c6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion (stance)\n",
      " [[ 14   0   1]\n",
      " [158   0   3]\n",
      " [ 24   0   0]] \n",
      "Confusion (PI)\n",
      " [[ 19   0   1]\n",
      " [157   0   1]\n",
      " [ 22   0   0]] \n",
      "stance F1 0.044   pi F1 0.058\n",
      "\n",
      "🟢 Epoch 6/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1d5490e92c4d4bba0259148bb225af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion (stance)\n",
      " [[ 14   0   1]\n",
      " [159   0   2]\n",
      " [ 24   0   0]] \n",
      "Confusion (PI)\n",
      " [[ 19   0   1]\n",
      " [157   0   1]\n",
      " [ 22   0   0]] \n",
      "stance F1 0.044   pi F1 0.058\n",
      "\n",
      "🟢 Epoch 7/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32386bb93c7842bcb61aed3d8b41036f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion (stance)\n",
      " [[ 14   0   1]\n",
      " [159   0   2]\n",
      " [ 24   0   0]] \n",
      "Confusion (PI)\n",
      " [[ 20   0   0]\n",
      " [157   0   1]\n",
      " [ 22   0   0]] \n",
      "stance F1 0.044   pi F1 0.061\n",
      "🔴 early stop – no improvement\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════\n",
    "# 0)  ENV  ─ set before torch / transformers\n",
    "# ═══════════════════════════════════════════════════\n",
    "import os, warnings, random, gc, numpy as np\n",
    "os.environ.pop(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\", None)\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.9\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ═══════════════════════════════════════════════════\n",
    "# 1)  LIBS\n",
    "# ═══════════════════════════════════════════════════\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import AdamW\n",
    "from transformers import (AutoTokenizer, AutoModel,\n",
    "                          get_cosine_schedule_with_warmup)\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm.auto import tqdm\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\")  if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available()        else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(\"Running on ➜\", device)\n",
    "\n",
    "# ═══════════════════════════════════════════════════\n",
    "# 2)  HYPER‑PARAMS   (updated per instructions)\n",
    "# ═══════════════════════════════════════════════════\n",
    "MODEL_NAME   = \"microsoft/deberta-v3-large\"\n",
    "EPOCHS       = 30\n",
    "PATIENCE     = 6\n",
    "BATCH        = 16        # micro‑batch\n",
    "ACC_STEPS    = 2         # 16×2 ⇒ eff‑batch = 32\n",
    "LR_BACKBONE  = 3e-6\n",
    "LR_HEADS     = 3e-5      # 10 × faster\n",
    "MAX_NORM     = 5.0\n",
    "WARMUP_FRAC  = .1\n",
    "GAMMA        = 2.0       # focal‑loss γ\n",
    "NUM_LABELS   = 3\n",
    "LABELS       = np.arange(NUM_LABELS)\n",
    "\n",
    "# ═══════════════════════════════════════════════════\n",
    "# 3)  TOKENISER  (resize‑tokens only)\n",
    "# ═══════════════════════════════════════════════════\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "tok.add_special_tokens(\n",
    "    {\"additional_special_tokens\": [\"<CONTEXT>\", \"</CONTEXT>\",\n",
    "                                   \"<REPLY>\", \"</REPLY>\"]})\n",
    "\n",
    "# ═══════════════════════════════════════════════════\n",
    "# 4)  DATA  – load cache, pin once on GPU\n",
    "# ═══════════════════════════════════════════════════\n",
    "cache = np.load(\"cached_encodings.npz\", mmap_mode=\"r\")\n",
    "IDS   = torch.from_numpy(cache[\"ids\"   ]).to(device, non_blocking=True)\n",
    "MASK  = torch.from_numpy(cache[\"mask\"  ]).to(device, non_blocking=True)\n",
    "Y_S   = torch.from_numpy(cache[\"stance\"]).to(device)\n",
    "Y_P   = torch.from_numpy(cache[\"pi\"    ]).to(device)\n",
    "\n",
    "class GPUSliceSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids, mask, ys, yp):\n",
    "        self.ids, self.mask, self.ys, self.yp = ids, mask, ys, yp\n",
    "    def __len__(self): return self.ids.size(0)\n",
    "    def __getitem__(self, i):\n",
    "        return {\"input_ids\":      self.ids [i],\n",
    "                \"attention_mask\": self.mask[i],\n",
    "                \"stance_label\":   self.ys  [i],\n",
    "                \"pi_label\":       self.yp  [i]}\n",
    "\n",
    "full_ds           = GPUSliceSet(IDS, MASK, Y_S, Y_P)\n",
    "train_ds, val_ds  = random_split(full_ds, [int(.8*len(full_ds)),\n",
    "                                           len(full_ds)-int(.8*len(full_ds))])\n",
    "train_idx_gpu = torch.tensor(train_ds.indices, device=device)\n",
    "\n",
    "# ═══════════════════════════════════════════════════\n",
    "# 5)  SAMPLER  (balanced – new shuffle each epoch)\n",
    "# ═══════════════════════════════════════════════════\n",
    "def make_sampler(epoch_seed: int) -> WeightedRandomSampler:\n",
    "    torch.manual_seed(epoch_seed)\n",
    "    lbl_cpu = Y_S[train_idx_gpu].to(\"cpu\", dtype=torch.long)\n",
    "    cnt     = torch.bincount(lbl_cpu, minlength=NUM_LABELS).float()\n",
    "    w       = 1.0 / cnt.clamp(min=1)\n",
    "    weights = w[lbl_cpu]\n",
    "    return WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "# ═══════════════════════════════════════════════════\n",
    "# 6)  MODEL  – LoRA r = 32, α = 64, + out_proj, bf16\n",
    "# ═══════════════════════════════════════════════════\n",
    "def build_backbone():\n",
    "    base = AutoModel.from_pretrained(MODEL_NAME, low_cpu_mem_usage=True)\n",
    "    base.gradient_checkpointing_enable()\n",
    "    cfg  = LoraConfig(\n",
    "            task_type      = TaskType.FEATURE_EXTRACTION,\n",
    "            r              = 32,\n",
    "            lora_alpha     = 64,\n",
    "            lora_dropout   = .05,\n",
    "            target_modules = [\"query_proj\", \"value_proj\", \"out_proj\"])\n",
    "    base = get_peft_model(base, cfg)\n",
    "    base.resize_token_embeddings(len(tok))\n",
    "    return base\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = build_backbone()\n",
    "        h = self.backbone.config.hidden_size\n",
    "        self.head_s  = nn.Linear(h, NUM_LABELS)\n",
    "        self.head_pi = nn.Linear(h, NUM_LABELS)\n",
    "    def forward(self, ids, mask):\n",
    "        with torch.autocast(device.type, torch.bfloat16):\n",
    "            cls = self.backbone(ids, attention_mask=mask).last_hidden_state[:, 0]\n",
    "        return self.head_s(cls), self.head_pi(cls)\n",
    "\n",
    "model = MultiHead().to(device)\n",
    "print(); model.backbone.print_trainable_parameters()\n",
    "\n",
    "# separate param groups\n",
    "backbone_params = list(model.backbone.parameters())\n",
    "head_params     = list(model.head_s.parameters()) + list(model.head_pi.parameters())\n",
    "\n",
    "# ═══════════════════════════════════════════════════\n",
    "# 7)  FOCAL‑LOSSES  (γ = 2)\n",
    "# ═══════════════════════════════════════════════════\n",
    "def focal_loss(alpha, gamma):\n",
    "    class _F(nn.Module):\n",
    "        def __init__(self): super().__init__(); self.a=alpha\n",
    "        def forward(self, logit, tgt):\n",
    "            ce = F.cross_entropy(logit, tgt, weight=self.a, reduction=\"none\")\n",
    "            return ((1.0 - torch.exp(-ce))**gamma * ce).mean()\n",
    "    return _F()\n",
    "\n",
    "w_s = torch.tensor(compute_class_weight(\"balanced\",\n",
    "                                        classes=LABELS,\n",
    "                                        y=Y_S[train_idx_gpu].cpu().numpy()),\n",
    "                   dtype=torch.float, device=device)\n",
    "w_p = torch.tensor(compute_class_weight(\"balanced\",\n",
    "                                        classes=LABELS,\n",
    "                                        y=Y_P[train_idx_gpu].cpu().numpy()),\n",
    "                   dtype=torch.float, device=device)\n",
    "\n",
    "loss_s = focal_loss(w_s, GAMMA)\n",
    "loss_p = focal_loss(w_p, GAMMA)\n",
    "\n",
    "# ═══════════════════════════════════════════════════\n",
    "# 8)  OPTIMISER & SCHEDULER  (two‑tier LR)\n",
    "# ═══════════════════════════════════════════════════\n",
    "optim = AdamW([\n",
    "        {\"params\": backbone_params, \"lr\": LR_BACKBONE},\n",
    "        {\"params\": head_params,     \"lr\": LR_HEADS    }],\n",
    "        betas=(0.9,0.999), weight_decay=0.01)\n",
    "\n",
    "steps_per_epoch = len(train_ds) // BATCH\n",
    "total_steps     = steps_per_epoch * EPOCHS\n",
    "sched = get_cosine_schedule_with_warmup(\n",
    "    optim, int(WARMUP_FRAC * total_steps), total_steps, num_cycles=2)\n",
    "\n",
    "# ═══════════════════════════════════════════════════\n",
    "# 9)  TRAIN / EVAL\n",
    "# ═══════════════════════════════════════════════════\n",
    "def train_one_epoch(ep):\n",
    "    sampler = make_sampler(ep)\n",
    "    loader  = DataLoader(train_ds, batch_size=BATCH,\n",
    "                         sampler=sampler, drop_last=True)\n",
    "    model.train(); running=0.\n",
    "    bar = tqdm(loader, desc=\"train\", leave=False)\n",
    "    for step, b in enumerate(bar, 1):\n",
    "        ids,mask = b[\"input_ids\"], b[\"attention_mask\"]\n",
    "        ys,yp    = b[\"stance_label\"], b[\"pi_label\"]\n",
    "        s,p      = model(ids,mask)\n",
    "        loss     = (loss_s(s,ys)+loss_p(p,yp))/ACC_STEPS\n",
    "        loss.backward()\n",
    "        if step % ACC_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_NORM)\n",
    "            optim.step(); sched.step(); optim.zero_grad(set_to_none=True)\n",
    "        running += loss.item()*ACC_STEPS\n",
    "        bar.set_postfix(loss = running/step)\n",
    "    return running/step\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    loader = DataLoader(val_ds, batch_size=BATCH)\n",
    "    model.eval(); ps_s,ps_p,ls_s,ls_p=[],[],[],[]\n",
    "    for b in loader:\n",
    "        s,p = model(b[\"input_ids\"], b[\"attention_mask\"])\n",
    "        ps_s.append(s.argmax(-1).cpu()); ls_s.append(b[\"stance_label\"].cpu())\n",
    "        ps_p.append(p.argmax(-1).cpu()); ls_p.append(b[\"pi_label\"].cpu())\n",
    "    ps_s,ls_s = torch.cat(ps_s),torch.cat(ls_s)\n",
    "    ps_p,ls_p = torch.cat(ps_p),torch.cat(ls_p)\n",
    "\n",
    "    cm_s = confusion_matrix(ls_s, ps_s, labels=LABELS)\n",
    "    cm_p = confusion_matrix(ls_p, ps_p, labels=LABELS)\n",
    "    f1_s = f1_score(ls_s, ps_s, average=\"macro\")\n",
    "    f1_p = f1_score(ls_p, ps_p, average=\"macro\")\n",
    "\n",
    "    print(\"\\nConfusion (stance)\\n\", cm_s,\n",
    "          \"\\nConfusion (PI)\\n\", cm_p,\n",
    "          f\"\\nstance F1 {f1_s:.3f}   pi F1 {f1_p:.3f}\")\n",
    "\n",
    "    return f1_s+f1_p, f1_s, f1_p\n",
    "\n",
    "def clear_mps():\n",
    "    if device.type==\"mps\": torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# ═══════════════════════════════════════════════════\n",
    "# 10)  LOOP\n",
    "# ═══════════════════════════════════════════════════\n",
    "writer,best,wait = SummaryWriter(), -1e9, 0\n",
    "for ep in range(1,EPOCHS+1):\n",
    "    print(f\"\\n🟢 Epoch {ep}/{EPOCHS}\")\n",
    "    tr_loss = train_one_epoch(ep)\n",
    "    score,f1_s,f1_p = evaluate()\n",
    "\n",
    "    writer.add_scalar(\"loss/train\", tr_loss, ep)\n",
    "    writer.add_scalars(\"val\", {\"stance_f1\":f1_s, \"pi_f1\":f1_p}, ep); writer.flush()\n",
    "\n",
    "    if score > best: best,wait = score,0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= PATIENCE:\n",
    "            print(\"🔴 early stop – no improvement\"); break\n",
    "    clear_mps()\n",
    "\n",
    "writer.close(); clear_mps()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
